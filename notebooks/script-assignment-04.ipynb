{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Script Assignment 4\n",
    "\n",
    "## Group 4\n",
    "\n",
    "## Assignment\n",
    "\n",
    "You are working as a data scientist for a financial technology company specializing in credit risk assessment. Your task is to build and evaluate an ensemble model using historical loan data to predict the likelihood of default for new loan applicants.\n",
    "\n",
    "- Load the historical loan dataset (`loan_data.csv`) containing features such\n",
    " as credit score, income, loan amount, and default status. Preprocess the data by handling missing values, encoding categorical variables, and splitting the dataset into training and testing sets.\n",
    "- Implement three different ensemble models: Random Forest, Gradient \n",
    "Boosting, and Voting Classifier. Train each ensemble model on the training dataset and evaluate its performance using appropriate evaluation metrics for classification tasks (e.g., accuracy, precision, recall, F1-score, ROC-AUC).\n",
    "- Compare the performance of the three ensemble models and select the \n",
    "best-performing model based on evaluation metrics. Provide insights into why the selected ensemble model might be well-suited for credit risk assessment in fintech.\n",
    "\n",
    "## Links\n",
    "\n",
    "- [Kaggle Loan Default Dataset](https://www.kaggle.com/datasets/yasserh/loan-default-dataset/data)\n",
    "- [Data Cleaning and Preprocessing Tactics](https://www.kaggle.com/code/nkitgupta/advance-data-preprocessing)\n",
    "\n",
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8b3fa7499904268"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the dataset "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68ada97a638ff036"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = '../data/loan_data.csv'\n",
    "df_loan_data = pd.read_csv(file_path)\n",
    "df_loan_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e67d89a893289e7d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Review the dataframe and visualize missing data\n",
    "\n",
    "In this section, we will review the dataframe's structure and visualize the missing data.  We will also create a function to determine which columns have missing data and the percentage of missing data in each column.\n",
    "\n",
    "### Review the dataframe"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9187eeb32de5c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_loan_data.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b35a809e6505069",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate missing data percentages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21f74ae5326e8918"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_missing_percentages(df):\n",
    "    \"\"\"Calculate the percentage of missing data in each column of a dataframe.\"\"\"\n",
    "    total = df.shape[0]\n",
    "    missing_columns = [col for col in df.columns if df[col].isnull().sum() > 0]\n",
    "    miss_pct = {}\n",
    "    for col in missing_columns:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        per = (null_count/total) * 100\n",
    "        miss_pct[col] = per\n",
    "        print(f\"{col}: {null_count} ({per:.3f}%)\")\n",
    "    return miss_pct"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "982159779ca372f1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "_ = calculate_missing_percentages(df_loan_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f61f825df27f3d7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize missing data\n",
    "\n",
    "The `missingno` library provides a matrix visualization of the missing data, and the `matplotlib` library provides a bar chart of the missing data percentages."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbee6063284db695"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "msno.matrix(df_loan_data)\n",
    "plt.figure(figsize = (15,9))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64a4856956edf29d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "From both the chart and statistics above, there are a few columns with substantial missing data. We will need to address this missing data before we can proceed with building the ensemble models.\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "In this section, we will review features, handle the missing data, encode the categorical variables, and split the dataset into training and testing sets.\n",
    "\n",
    "### Review features\n",
    "\n",
    "We'll start by reviewing the features in the dataset and removing any that we know have little to no impact on the model.  \n",
    "\n",
    "According to an article on forbes.com, the following features are important for credit risk assessment:\n",
    "\n",
    "- Credit Score and History\n",
    "- Income\n",
    "- Debt-to-income Ratio\n",
    "- Collateral\n",
    "- Origination Fee\n",
    "\n",
    "In order to simplify the dataset, we'll remove the following columns: \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "682a94687427a042"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_loan_data.drop(['loan_limit','Gender', 'approv_in_adv','loan_type', 'loan_purpose', 'Credit_Worthiness','open_credit',\n",
    "        'business_or_commercial', 'rate_of_interest', 'Interest_rate_spread', 'Neg_ammortization', 'interest_only',\n",
    "        'lump_sum_payment', 'construction_type', 'occupancy_type', 'Secured_by', 'total_units', 'credit_type',\n",
    "        'co-applicant_credit_type', 'submission_of_application', 'Region', 'Security_Type', 'ID', 'year'], axis = 1, inplace = True)\n",
    "\n",
    "df_loan_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e970c8660006fba9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handle missing data\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e55bee3ed5838ac7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Review the missing data percentages\n",
    "_ = calculate_missing_percentages(df_loan_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12715b189ede6ef9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Handle missing numerical data\n",
    "\n",
    "We will use the SimpleImputer class to fill in the missing data.  We will use the mean value for numerical columns and the most frequent value for categorical columns."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd6b515dd1f4a85b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Only numerical features\n",
    "num_cols = [col for col in df_loan_data.columns if df_loan_data[col].dtype != 'object']\n",
    "print(num_cols)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e990d4cc07d50ec5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "# Run SimpleImputer on a subset of columns on df_loan_data\n",
    "for col in num_cols:\n",
    "    df_loan_data[col] = imputer.fit_transform(df_loan_data[[col]])\n",
    "\n",
    "df_loan_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "174bc7270cd7cd7c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_loan_data.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb405dbcf8366557",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Handle missing categorical variables\n",
    "\n",
    "Now we'll set our sights on cleaning up the categorical variables.  We'll start by identifying the categorical variables and then filling in the missing data with the most frequent value."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dc94c0c94bcf277"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_loan_data.dropna(inplace = True)\n",
    "df_loan_data.isnull().sum()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "402615d617250261",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encode categorical variables\n",
    "\n",
    "We will use the LabelEncoder class to encode the age variable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbbe5af54731f5f9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df_loan_data['age'] = label_encoder.fit_transform(df_loan_data['age'])\n",
    "df_loan_data['age']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4b5a1cb1c4d12b1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split the dataset into training and testing sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9adb7c3dda036ff2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X = df_loan_data.drop('Status', axis=1)\n",
    "y = df_loan_data['Status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a601204f0d40d102",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
